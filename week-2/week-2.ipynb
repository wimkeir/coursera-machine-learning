{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with multiple variables\n",
    "\n",
    "Also called **multivariate linear regression**.\n",
    "\n",
    "Variables also called **features**.\n",
    "\n",
    "> **Notation**\n",
    ">\n",
    "> $m$ = number of training examples (feature vectors)\n",
    ">\n",
    "> $n$ = number of features in each example (dimension of feature vectors)\n",
    ">\n",
    "> $x^{(i)}$ = input of $i^{th}$ training example (feature vector)\n",
    ">\n",
    "> $x^{(i)}_{j}$ = value of feature $j$ in $i^{th}$ training example (feature vector)\n",
    "\n",
    "## Hypothesis (model) for multivariate linear regression\n",
    "\n",
    "> **Hypothesis for linear regression**\n",
    ">\n",
    "> Univariate: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$ ($n = 1$)\n",
    ">\n",
    "> Multivariate: $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\ldots + \\theta_{n}x_{n}$ (arbitrary $n$)\n",
    "\n",
    "Let us assume that $x^{i}_{0} = 1$ for all $i \\in [1, m]$. This allows us to write $x$ as\n",
    "\n",
    "$$x = \\begin{bmatrix}x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix} = \\begin{bmatrix}1 \\\\ x_{1} \\\\ \\vdots \\\\ x_{n}\\end{bmatrix} \\in \\mathcal{R}^{n+1}$$\n",
    "\n",
    "in other words, our training data is a set of $n+1$-dimensional vectors. Writing our hypothesis parameters as the vector\n",
    "\n",
    "$$\\theta = \\begin{bmatrix}\\theta_{0} \\\\ \\theta_{1} \\\\ \\vdots \\\\ \\theta_{n}\\end{bmatrix} \\in \\mathcal{R}^{n+1}$$\n",
    "\n",
    "allows us to simplify our hypothesis function as the vector product\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta^{T}x$$\n",
    "\n",
    "This product expands to the linear combination\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\ldots + \\theta_{n}x_{n}$$\n",
    "\n",
    "We can think of $\\theta_{0}$ as the \"base value\", and of the remaining $\\theta_{i}$ as the weightings of each corresponding feature in the inputs to the final output value.\n",
    "\n",
    "## Gradient descent for multivariate linear regression\n",
    "\n",
    "\n",
    "Recall the **mean squared error** cost function\n",
    "\n",
    "$$J(\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{n}) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2}$$\n",
    "\n",
    "which we can simplify as\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2}$$\n",
    "\n",
    "where $\\theta \\in \\mathcal{R}^{n+1}$.\n",
    "\n",
    "Recall also the algorithm for gradient descent, which is to *repeat the following until convergence* for $j \\in [0, n]$\n",
    "\n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)$$\n",
    "\n",
    "The *partial derivative term* becomes\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)}) - y^{(i)}})x^{(i)}_{j}$$\n",
    "\n",
    "Which gives us the **gradient descent algorithm for multivariate linear regression**, which is to *repeat the following until convergence* for $j \\in [0, n]$\n",
    "\n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)}) - y^{(i)}})x^{(i)}_{j}$$\n",
    "\n",
    "## Gradient descent in practice\n",
    "\n",
    "There are many practical ways to improve the efficiency of gradient descent in practice.\n",
    "\n",
    "### Feature scaling\n",
    "\n",
    "Gradient descent converges more quickly when features are on a similar scale. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, therefore oscillating inefficiently down to an optimum when the ranges are very uneven.\n",
    "\n",
    "For this reason we can **scale features** to take on similar value ranges. Ideally we want features roughly in the range\n",
    "\n",
    "$$-1 \\le x_{j} \\le 1$$\n",
    "\n",
    "### Mean normalization\n",
    "\n",
    "Replace features $x_{i}$ with $x_{i} - \\mu_{i}$ (except $x_{0}$) to make features have approximately zero mean.\n",
    "\n",
    "In general **feature scaling** and **mean normalization** means we transform features as\n",
    "\n",
    "$$x_{j} \\leftarrow \\frac{x_{j} - \\mu_{j}}{s_{j}}$$\n",
    "\n",
    "Where\n",
    "- $\\mu_{j}$ is the average of feature $x_{j}$\n",
    "- $s_{j}$ scales the feature $j$ to a range close to $-1 \\le x_{j} \\le 1$\n",
    "\n",
    "$s_{j}$ is often the value $max(x_{j}) - min(x_{j})$ or the *standard deviation* of $x_{j}$\n",
    "\n",
    "### \"Debugging\" gradient descent\n",
    "\n",
    "Recall that we seek $\\underset{\\theta}{minimum}\\hspace{2mm}J(\\theta)$.\n",
    "\n",
    "It is often useful to plot the values of $J(\\theta)$ against the number of iterations of the gradient descent algorithm. \n",
    "\n",
    "The idea is that we should see the value of $J(\\theta)$ decreasing as the number of iterations increase. The rate of decrease is a good indication of the efficiency of the algorithm.\n",
    "\n",
    "We can use a convergence test to declare convergence if the value of $J(\\theta)$ decreases by less than a given threshold - this threshold, however, can be difficult to choose. A plot can usually give a better idea of whether the algorithm is converging or not.\n",
    "\n",
    "If $J(\\theta)$ is *increasing* as the iterations increase (ie. *diverging*), or it goes up and down in oscillations, it's usually a sign to choose a smaller learning rate $\\alpha$.\n",
    "\n",
    "> For sufficiently small $\\alpha$, $J(\\theta)$ should decrease on every iteration (eventually converging)\n",
    ">\n",
    "> But choosing an $\\alpha$ too small results in slow convergence\n",
    "\n",
    "### Polynomial regression\n",
    "\n",
    "Sometimes we can imagine that a polynomial function might fit the training data better. We can map features from a polynomial hypothesis (model) to a linear one.\n",
    "\n",
    "If for example $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} + \\theta_{3}x^{3}$, we can let\n",
    "- $x_{1} = x$\n",
    "- $x_{2} = x^{2}$\n",
    "- $x_{3} = x^{3}$\n",
    "\n",
    "Which gives us a linear hypothesis $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3}$.\n",
    "\n",
    "Feature scaling becomes incredibly important here, as ranges can be raised to exponents.\n",
    "\n",
    "Exponents don't always need to be in increasing, discrete ranges. We can use roots etc. as well to manipulate the features to better fit the training data.\n",
    "\n",
    "We can also *combine* features to create new features. For example, given two features $x_{1}$ and $x_{2}$, we can combine these to get a third feature $x_{3}$.\n",
    "\n",
    "# Normal equation\n",
    "\n",
    "> **Gradient descent**\n",
    ">\n",
    "> We have a model $h_{\\theta}$ with parameters $\\theta$\n",
    ">\n",
    "> The model has a cost function $J(\\theta)$ that we seek to minimize\n",
    ">\n",
    "> Gradient descent is an **iterative** algorithm for finding a $\\theta$ that minimizes $J(\\theta)$\n",
    "\n",
    "The **normal equation** is an algorithm for solving $\\theta$ **analytically**.\n",
    "\n",
    "\n",
    "## Intuition\n",
    "\n",
    "The cost function $J(\\theta)$ has a minimum where its derivative is zero. We can find this minimum by solving for $\\theta$ in\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\theta}J(\\theta) = 0$$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "We construct an $m \\times (n + 1)$ **design matrix $X$** out of the training set as follows\n",
    "\n",
    "$$X = \\begin{bmatrix}x^{(1)} \\\\ x^{(2)} \\\\ \\vdots \\\\ x^{(m)}\\end{bmatrix} \\in \\mathcal{R}^{m \\times (n + 1)}$$\n",
    "\n",
    "where $m$ is the number of $n + 1$-dimensional feature vectors\n",
    "\n",
    "$$x^{(i)} = \\begin{bmatrix}1 & x_{1} & x_{2} & \\ldots & x_{n}\\end{bmatrix} \\in \\mathcal{R}^{n + 1}$$\n",
    "\n",
    "We also construct an $m$-dimensional vector $y$ out of the training set outputs\n",
    "\n",
    "$$y = \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix} \\in \\mathcal{R}^{m}$$\n",
    "\n",
    "The **solution $\\theta$** is given by\n",
    "\n",
    "$$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "Or in Octave:\n",
    "```octave\n",
    "# `pinv` calculates the (numerical) pseudo-inverse of any matrix\n",
    "# `inv` calculates the actual inverse if the matrix is invertible\n",
    "pinv(X'*X)*X'*y\n",
    "```\n",
    "\n",
    "## Comparison with gradient descent\n",
    "\n",
    "Unlike *gradient descent*, there is no need for **feature scaling** with the *normal equation*.\n",
    "\n",
    "Advantages of **gradient descent**:\n",
    "- Works well even when $n$ is large (many features)\n",
    "\n",
    "Disadvantages of **gradient descent**:\n",
    "- Need to choose learning rate $\\alpha$\n",
    "- Requires many iterations\n",
    "\n",
    "Advantages of **normal equation**:\n",
    "- Easy to implement\n",
    "\n",
    "Disadvantages of **normal equation**\n",
    "- Computing $(X^{T}X)^{-1} \\in \\mathcal{R}^{n \\times n}$ is slow if $n$ is large (matrix inversion is cubic in $n$ ie. $O(n^{3})$\n",
    "\n",
    "> **Conclusion**: use *normal equation* if $n$ is reasonably small ($\\le$ 10 000), use *gradient descent* when $n$ is large\n",
    "\n",
    "## The normal equation and non-invertibility\n",
    "\n",
    "If $X^{T}X$ is non-invertible (singular / degenerate), we can still solve for $\\theta$ by numerically computing the psuedo-inverse $(X^{T}X)^{\\dagger}$ instead.\n",
    "\n",
    "See `inv` vs `pinv` in Octave.\n",
    "\n",
    "If $X^{T}X$ is non-invertible, it's likely that\n",
    "- Some features are redundant (the columns of $X$ are **linearly dependent**)\n",
    "- There are too many features ($m \\le n$)\n",
    "\n",
    "Options: either *remove features* or use *regularization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
